Technical Documentation: SEO Analysis Tool

1. Overall Architecture
=======================
The SEO Analysis Tool is a desktop application built with Python and Tkinter for the GUI. It aims to provide a comprehensive suite of tools for analyzing website SEO performance, integrating on-page analysis, external data sources (Search Console, SEMrush via CSV), and AI-powered content generation (via LM Studio).

The architecture consists of:
- A main GUI application (multiple versions exist, e.g., `main.py`, `seo_main_complete.py`) that orchestrates the workflow.
- Backend modules (`seo_*.py`) responsible for specific tasks like crawling, content loading, various analyses, data handling, and reporting.
- Configuration files (`seo_config.py`, `seo_config_settings.py`) for managing settings.
- Data storage directories (`data`, `logs`, `reports`, `sessions`).
- A separate, simpler URL validation utility (`url_validator_app.py`, `url_validator.py`, `config.py`).

2. Core Modules (`seo_*.py`)
============================

*   **`seo_config.py` & `seo_config_settings.py`**:
    *   Purpose: Define constants and configuration parameters for the application.
    *   Content: Includes UI settings (window size, fonts, colors), crawling parameters (limits, user agent), analysis thresholds (meta description length, image size), API endpoints (LM Studio), file paths, and CSV column mappings.
    *   Note: There appears to be overlap and potential redundancy between these two files. `seo_config_settings.py` seems to hold more granular settings for specific modules.

*   **`seo_url_validator.py`**:
    *   Purpose: Provides utility functions for URL validation, normalization, and classification within the main SEO tool.
    *   Key Class: `SEOURLValidator` (static methods).
    *   Functionality: Checks basic validity, normalizes URLs (lowercase, default ports, trailing slashes), extracts components (domain, path), checks internal/external status, identifies file types (image, CSS, JS, PDF, etc.) based on extensions. Uses `urllib.parse`.

*   **`seo_crawler.py`**:
    *   Purpose: Handles website crawling and sitemap processing.
    *   Key Classes:
        *   `SEOCrawler`: Performs breadth-first crawling, respects limits (pages, depth), parses HTML (`BeautifulSoup`), follows internal HTML links, avoids non-HTML/external links, records visited URLs and errors. Uses `requests`.
        *   `SitemapProcessor`: Fetches and parses XML sitemaps (`.xml`, `.xml.gz`), handles sitemap indexes recursively, extracts URLs. Uses `requests`, `xml.etree.ElementTree`, `gzip`.
    *   Dependencies: `seo_url_validator`, `seo_config_settings`.

*   **`seo_content_loader.py`**:
    *   Purpose: Fetches HTML content for a list of URLs efficiently.
    *   Key Class: `ContentLoader`.
    *   Functionality: Primarily uses `asyncio` and `aiohttp` for concurrent fetching. Stores content, status codes, content types, and lengths. Provides helpers (`get_soup`, `get_title`, `get_meta_description`, `get_h1`) using `BeautifulSoup`. Includes a synchronous fallback using `requests` and `concurrent.futures`.

*   **`seo_meta_description.py`**:
    *   Purpose: Analyzes meta descriptions of fetched pages.
    *   Key Class: `MetaDescriptionAnalyzer`.
    *   Functionality: Uses `ContentLoader` to get meta descriptions. Checks for presence, length (against configured min/max), and assigns quality (`missing`, `too_short`, `too_long`, `good`). Provides results summary and DataFrame conversion.
    *   Dependencies: `seo_config_settings`.

*   **`seo_images.py`**:
    *   Purpose: Analyzes images found on fetched pages.
    *   Key Class: `ImageAnalyzer`.
    *   Functionality: Uses `ContentLoader` (`get_soup`) to find `<img>` tags. Checks for `alt` text. Identifies format via extension. Attempts to get image size using `requests` (`HEAD` then `GET`). Provides results summary and DataFrame conversion.

*   **`seo_links.py`**:
    *   Purpose: Analyzes hyperlinks (`<a>` tags) on fetched pages.
    *   Key Class: `LinksAnalyzer`.
    *   Functionality: Uses `ContentLoader` (`get_soup`) to find `<a>` tags. Resolves relative URLs. Classifies links (internal/external, nofollow). Checks for broken links using synchronous `requests.head`. Provides results summary and DataFrame conversion.
    *   Dependencies: `seo_url_validator`.

*   **`seo_reachability.py`**:
    *   Purpose: Analyzes the internal link structure and page reachability.
    *   Key Class: `ReachabilityAnalyzer`.
    *   Functionality: Builds a directed graph (`networkx.DiGraph`) from internal links provided by `LinksAnalyzer`. Identifies reachable URLs and orphan pages starting from the homepage. Calculates click depth (shortest path length) from the homepage. Provides results summary and DataFrame conversion.
    *   Dependencies: `networkx`, `seo_url_validator`.

*   **`seo_main_topic.py`**:
    *   Purpose: Identifies main topics and keywords for pages using NLP.
    *   Key Class: `MainTopicAnalyzer`.
    *   Functionality: Extracts text (title, meta, headings, body) using `ContentLoader` (`get_soup`). Preprocesses text (lowercase, punctuation removal, tokenization, stopword removal, lemmatization) using `nltk`. Extracts keywords using `sklearn.feature_extraction.text.TfidfVectorizer`. Identifies topics using `spacy` (NER, noun chunks). Calculates overall topic distribution. Provides results summary and DataFrame conversion. Downloads `nltk` and `spacy` resources if needed.
    *   Dependencies: `nltk`, `spacy`, `sklearn`.

*   **`seo_search_console.py`**:
    *   Purpose: Handles loading, analysis, and comparison of Google Search Console data (from CSV).
    *   Key Classes:
        *   `SearchConsoleData`: Loads and cleans SC CSV data using `pandas`. Aggregates metrics by query and landing page.
        *   `SearchConsoleAnalyzer`: Performs analysis on loaded SC data. Clusters queries (`TfidfVectorizer`, `KMeans`). Identifies topics for landing pages based on dominant query cluster impressions. Suggests internal links based on shared topics.
        *   `SearchConsoleComparison`: Compares two `SearchConsoleData` instances (old vs. new). Calculates changes in metrics for queries and landing pages. Identifies improved/declined items.
    *   Dependencies: `pandas`, `sklearn`.

*   **`seo_semrush.py`**:
    *   Purpose: Handles loading and analysis of SEMrush data (from CSV).
    *   Key Classes:
        *   `SEMrushData`: Loads and cleans SEMrush CSV data using `pandas`. Aggregates metrics by keyword and URL.
        *   `SEMrushAnalyzer`: Performs analysis on loaded SEMrush data. Clusters keywords (`TfidfVectorizer`, `KMeans`). Identifies topics for URLs based on dominant keyword cluster traffic. Calculates visibility and traffic per topic. Suggests internal links based on shared topics.
    *   Dependencies: `pandas`, `sklearn`.

*   **`seo_lmstudio.py`**:
    *   Purpose: Integrates with a local LM Studio instance for LLM tasks.
    *   Key Classes:
        *   `LMStudioClient`: Handles API communication (`requests`) with LM Studio endpoints (`/completions`, `/chat/completions`). Manages parameters. Checks connection.
        *   `MetaDescriptionGenerator`: Uses `LMStudioClient` to generate meta descriptions based on page title and content excerpt via prompted requests. Includes batch processing.
        *   `ContentGenerator`: Uses `LMStudioClient` to generate article content or "ADS" questions based on topic and keywords via prompted requests.
    *   Dependencies: `requests`.

*   **`seo_report.py`**:
    *   Purpose: Generates analysis reports in DOCX format.
    *   Key Class: `ReportGenerator`.
    *   Functionality: Uses `python-docx` to create reports. Provides methods to generate different report types (SEO, SC, SEMrush, Comparison, Final) by combining data from various analyzer objects. Structures reports with titles, summaries, tables, and visualizations (e.g., `wordcloud`). Includes basic styling and recommendations. Outputs timestamped DOCX files to the `reports` directory. Includes `print` statements for progress indication during generation.
    *   Dependencies: `python-docx`, `pandas`, `matplotlib`, `wordcloud`.

3. Main Application (GUI)
=========================
Multiple main application files exist:
- `main.py`, `seo_main_complete.py`, `seo_main_fixed.py`: These appear to be functional versions of the main Tkinter GUI. They create a tabbed interface (`ttk.Notebook`) for different functions (Crawling, Analysis, SC, SEMrush, Reports, LM Studio/Session). They instantiate and orchestrate the backend modules based on user actions (button clicks, file loading). They use `threading` for background tasks and display results/status updates. `seo_main.py` and `seo_main_complete.py` seem most complete.
- `seo_main_core.py`: Appears to be an incomplete skeleton or blueprint with method stubs.

The main GUI likely uses `seo_config.py` and/or `seo_config_settings.py` for configuration.

4. Configuration
================
- `seo_config.py`: Contains general configuration settings (window properties, paths, some defaults).
- `seo_config_settings.py`: Contains more specific parameters used by individual modules (e.g., crawler limits, analysis thresholds, API settings). There is some overlap with `seo_config.py`.
- `config.py`: Used only by the separate `url_validator_app.py`. Contains UI settings and validation parameters for that specific app.

5. Data Handling
================
- Input: Website URL, CSV files (Search Console, SEMrush), Session files.
- Processing:
    - Web content is fetched by `ContentLoader` and parsed by `BeautifulSoup`.
    - CSV data is loaded and processed using `pandas`.
    - Analysis results are stored in instance variables within analyzer objects (often dictionaries or lists).
    - NLP processing uses `nltk`, `spacy`, and `sklearn`.
    - Link graph analysis uses `networkx`.
- Output: DOCX reports, Log files, Session files.
- Session Management: The main GUI allows saving and loading the application state (references to analyzer objects and data) using `pickle` into `.seo` files in the `sessions` directory.

6. External Integrations
========================
- Search Console / SEMrush: Data is imported via user-provided CSV files. No direct API integration is present.
- LM Studio: Interacts with a locally running LM Studio instance via its HTTP API (`http://localhost:1234/v1` by default) for generating meta descriptions and other content.

7. Reporting
============
The `ReportGenerator` class in `seo_report.py` uses the `python-docx` library to create detailed reports. Different methods assemble data from various analyzer objects into structured DOCX files, including summaries, tables, word clouds, and recommendations. Reports are saved with timestamps in the `reports` directory.

8. Testing (`test_*.py`)
========================
Unit tests are provided for report generation functions:
- `test_seo_report.py`: Tests basic on-page SEO report. Uses mock analyzer classes returning pandas DataFrames.
- `test_comparison_report.py`: Tests Search Console comparison report. Uses a mock comparison class.
- `test_final_report.py`: Tests the comprehensive final report. Uses mock classes for all analyzers (on-page, SC, SEMrush). Defines mock SC/SEMrush analyzers.
- `test_search_console_report.py`: Tests SC-specific report. Imports mock SC analyzer from `test_final_report.py`.
- `test_semrush_report.py`: Tests SEMrush-specific report. Imports mock SEMrush analyzer from `test_final_report.py`.

The tests focus on ensuring the report generation methods run without errors using mock data and produce an output file.

9. Standalone URL Validator Utility
===================================
- `url_validator_app.py`: A simple, separate Tkinter GUI for validating URL format and accessibility.
- `url_validator.py`: The backend logic for the validator app. Uses the `validators` library and `requests`.
- `config.py`: Configuration file specifically for this utility.

10. Key Dependencies
====================
- GUI: `tkinter` (`ttk`)
- Data Handling: `pandas`, `numpy`
- Web: `requests`, `aiohttp`, `urllib.parse`, `validators`
- HTML Parsing: `beautifulsoup4`
- NLP: `nltk`, `spacy` (`en_core_web_sm` model), `scikit-learn`
- Graph Analysis: `networkx`
- Reporting: `python-docx`, `matplotlib`, `wordcloud`
- Serialization: `pickle` (for sessions)
- XML Parsing: `xml.etree.ElementTree`
- Compression: `gzip`

11. File Structure
==================
- Root Directory: Contains Python scripts (`.py`), config files (`.txt`, `.md`), license, requirements.
- `data/`: Likely intended for storing persistent data (currently empty or unused based on analysis).
- `logs/`: Contains timestamped log files generated by the application's logging setup.
- `reports/`: Default output directory for generated `.docx` reports.
- `sessions/`: Default directory for saving/loading application state (`.seo` files via pickle).
